from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from peft import PeftModel
import torch

# 1. ëª¨ë¸ ê²½ë¡œ ì§€ì •
base_model = "mistralai/Mistral-7B-v0.1"
lora_model = "./outputs/final-model"  # LoRA í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜

# 2. í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(
    base_model, load_in_8bit=True, device_map="auto"
)
model = PeftModel.from_pretrained(model, lora_model)

# 3. ì±„íŒ… íŒŒì´í”„ë¼ì¸ êµ¬ì„±
chat = pipeline("text-generation", model=model, tokenizer=tokenizer)

# 4. ëŒ€í™” ë£¨í”„
print("ğŸ’¬ Chatbot ì‹œì‘! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)\n")
history = []

while True:
    user_input = input("ğŸ‘¤ You: ")
    if user_input.lower() == "exit":
        break

    # ê°„ë‹¨í•œ íˆìŠ¤í† ë¦¬ í¬í•¨ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = ""
    for line in history[-5:]:
        prompt += line + "\n"
    prompt += f"ìœ ì €: {user_input}\nì±—ë´‡:"

    response = chat(
        prompt, max_new_tokens=100, temperature=0.7, top_p=0.9, repetition_penalty=1.1
    )[0]["generated_text"]

    # ì‘ë‹µë§Œ ì¶”ì¶œ
    bot_reply = response.split("ì±—ë´‡:")[-1].strip().split("\n")[0]
    print(f"ğŸ¤– Bot: {bot_reply}")
    history.append(f"ìœ ì €: {user_input}")
    history.append(f"ì±—ë´‡: {bot_reply}")
